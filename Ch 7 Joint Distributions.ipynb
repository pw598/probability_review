{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba9624-9f6e-4084-b82c-00cbca314f8d",
   "metadata": {},
   "source": [
    "# Chapter 7: Joint Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf728fd-832f-45f3-949a-babd990cd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source of the content is freely available online\n",
    "# https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view\n",
    "# https://projects.iq.harvard.edu/stat110/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fc065-9b78-4a4f-aff9-b63ed0bd9326",
   "metadata": {},
   "source": [
    "<h4>Definition 7.1.1 (Joint CDF)</h4>\n",
    "\n",
    "The joint CDF of random variables $X$ and $Y$ is the function $F_{X,Y}$ given by:\n",
    "\n",
    "$F_{X,Y}(x,y) = P(X \\le x, Y \\le y)$\n",
    "\n",
    "The joint CDF of $n$ random variables is defined analogously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53286fde-d152-4648-93d6-e981a29cfc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "864318aa-c020-4461-9055-e88b8b992d4a",
   "metadata": {},
   "source": [
    "<h4>Definition 7.1.12 (Joint PMF)</h4>\n",
    "\n",
    "The joint PMF of discrete random variables $X$ and $Y$ is the function $p_{X,Y}$ given by:\n",
    "\n",
    "$P_{X,Y}(x,y) = P(X=x, Y=y)$\n",
    "\n",
    "The joint PMF of n discrete random variables is defined analogously.\n",
    "\n",
    "Just as univariate PMFs must be nonnegative and sum to 1, we require valid joint PMFs to be nonnegative and sum to 1, over all possible values of X and Y.\n",
    "\n",
    "$\\sum_x \\sum_y P(X=x, Y=y) = 1$\n",
    "\n",
    "The joint distribution can be used to find the problem of the event $(X,Y) \\in A$ for any set $A$ of points in the support of $(X,Y)$.\n",
    "\n",
    "$P((X,Y) \\in A) = \\sum_{x,y \\in A} \\sum p(X=x, Y=y)$\n",
    "\n",
    "From the joint distribution of $X$ and $Y$, we can get the distribution of $X$ alone by summing over the possible values of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45709d-0263-4afa-887e-bdc0f2ab24c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc2d485b-4380-4923-9801-3b82aca5a1af",
   "metadata": {},
   "source": [
    "<h4>Definition 7.1.3 (Marginal PMF)</h4>\n",
    "\n",
    "For discrete random variables $X$ and $Y$, the marginal PMF of $X$ is:\n",
    "\n",
    "$P(X=x) = \\sum_y P(X=x, Y=y)$\n",
    "\n",
    "The marginal PMF of $X$ is the PMF of $X$, viewing $X$ individually rather than jointly with $Y$.\n",
    "\n",
    "If we observe the value of $X$ and want to update our distribution of $Y$ to reflect this information, then instead of using the marginal PMF $P(Y=y)$, which does not take the information about $X$ into account, one should use a PMF that conditions on event $X=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d630fca-db5b-4ba8-8621-551f05654806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd60d030-03a1-4d56-9c4b-a55af73fda23",
   "metadata": {},
   "source": [
    "<h4>Definition 7.1.4 (Conditional PMF)</h4>\n",
    "\n",
    "For discrete random variables $X$ and $Y$, the conditional PMF of $Y$ given $X=x$ is:\n",
    "\n",
    "$P(Y=y | X=x) = \\frac{ P(X=x, Y=y) }{ P(X=x) }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0228808-a751-4582-ae05-99ddfca0dfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac86485e-0af0-47d2-951a-73160b1da076",
   "metadata": {},
   "source": [
    "<h4>Defintion 7.1.7 (Independence of Discrete Random Variables)</h4>\n",
    "\n",
    "Random variables $X$ and $Y$ are independent if for all $x$ and $y$:\n",
    "\n",
    "$F_{X,Y} (x,y) = F_X(x) F_Y(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a238e-44c7-4c25-930b-c4444315781b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf093dc7-2162-4354-bcaa-a3c9141c38ad",
   "metadata": {},
   "source": [
    "<h4>Definition 7.1.14 (Marginal PDF)</h4>\n",
    "\n",
    "For continuous random variables $X$ and $Y$ with joint PDF $f_{X,Y}$, the marginal PDF of $X$ is:\n",
    "\n",
    "$f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) ~dy$\n",
    "\n",
    "Marginalization works analogously with any number of variables. For example, if we have the joint PDF of $X$, $Y$, $Z$, and $W$, but want the joint PDF of $X$, $W$, we just have to integrate over all possible values of $Y$ and $Z$.\n",
    "\n",
    "$f_{X,W}(x,w) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X,Y,Z,W} (x,y,z,w) ~dy ~dz$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c4bcf-b35e-4089-a462-b40dca1d936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ad44d9f-764f-4789-8052-c1ba81cc5b04",
   "metadata": {},
   "source": [
    "<h4>Definition 7.1.15 (Conditional PMF)</h4>\n",
    "\n",
    "For continuous random variables $X$ and $Y$ with joint PDF $f_{X,Y}$, the conditional PDF of $Y$ given $X=x$ is:\n",
    "\n",
    "$F_{Y|X}(y|x) = \\frac{ f_{X,Y}(x,y) }{ f_X(x) }$\n",
    "\n",
    "for all $x$ with $f_X(x) \\gt 0$.\n",
    "\n",
    "We can recover the joint PDF $f_{X,Y}$ if we have the conditional PDF $f_{Y|X}$ and the corresponding marginal function:\n",
    "\n",
    "$f_{X,Y}(x,y) = f_{Y|X} (y|x) f_X(x)$\n",
    "\n",
    "Similarly, we can recover the joint PDF if we have $f_{X,Y}$ and $f_Y$:\n",
    "\n",
    "$f_{X,Y}(x,y) = f_{X|Y}(x|y) f_Y(y)$\n",
    "\n",
    "This allows us to develop continuous versions of Bayes' rule and LOTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f820a1-9eae-4b03-b4d1-2cdab80fec00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7479eb24-ba57-4195-b2c4-085ca0d9e357",
   "metadata": {},
   "source": [
    "<h4>Theorem 7.1.18 (Continuous Form of Bayes' Rule and LOTP)</h4>\n",
    "\n",
    "<h5>Bayes' Rule</h5>\n",
    "\n",
    "$f_{Y|X}(y|X) = \\frac{ f_{X,Y}(x|y) f_Y(y) }{f_X(x)}$\n",
    "\n",
    "for $f_X(x) > 0$.\n",
    "\n",
    "<h5>LOTP</h5>\n",
    "\n",
    "$f_X(x) = \\int_{-\\infty}^{\\infty f_{X|Y}(x|y) f_Y(y) ~dy}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bb5b9-8f4e-4b4d-b813-19ef485b1c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400aa715-c75c-43c5-8fe6-1c618cf4d766",
   "metadata": {},
   "source": [
    "<h4>Theorem 7.2.1 (2D LOTUS)</h4>\n",
    "\n",
    "Let $g$ be a function from $\\mathbb{R}^2$ to $\\mathbb{R}$. If $X$ and $Y$ are discrete, then:\n",
    "\n",
    "$E(g(X,Y)) = \\sum_x \\sum_y g(x,y) P(X=x,Y=y)$\n",
    "\n",
    "If $X$ and $Y$ are continuous, then:\n",
    "\n",
    "$E(g(X,Y)) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f_{X,Y}(x,y) ~dx ~dy$\n",
    "\n",
    "Like its 1D counterpart, this saves us from having to find the distribution of $g(X,Y)$ in order to calculate its expectation. Having the joint PMF or PDF of $X$ and $Y$ is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda84fb-7642-4d96-9e83-e2dce502157a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1178a1c4-166a-4c08-88cf-79d3336f4612",
   "metadata": {},
   "source": [
    "<h3>Theorem 7.3 Covariance and Correlation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67adfc6-49a7-4d06-9a77-5e0213a161bf",
   "metadata": {},
   "source": [
    "<h4>Definition 7.3.1 (Covariance)</h4>\n",
    "\n",
    "The covariance between $X$ and $Y$ is:\n",
    "\n",
    "$Cov(X,Y) = E( (X-EX)(Y-EY) ) = E(XY) - E(X)E(Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adbabc-b177-4d6e-878c-8e67bacede34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db29b962-2e25-4410-a91c-89f805d086f9",
   "metadata": {},
   "source": [
    "<h4>Theorem 7.4.4 (Multinomial Lumping)</h4>\n",
    "\n",
    "If $\\mathbf{X} \\text{~} Mult_k(n, \\mathbf{p})$, then for any distinct $i$ and $j$, $X_i + X_j \\text{~} Bin(n, p_i + p_j)$. The random vector of counts obtained from merging categories $i$ and $j$ is still Multinomial.\n",
    "\n",
    "$(X_1 + X_2, X_3, \\ldots, X_k) \\text{~} Multi_{k-1} (n, (p_1 + p_2, p_3, \\ldots, p_k))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32ce6d-3645-4979-b005-e6d2a67e63e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "880432ea-5df4-44fb-8341-941ba85e4b10",
   "metadata": {},
   "source": [
    "<h3>7.5 Multivariate Normal</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e5906-0cc8-4807-bca0-9eb1f8c85843",
   "metadata": {},
   "source": [
    "<h4>Definition 7.5.1 (Multivariate Normal Distribution)</h4>\n",
    "\n",
    "A $k$-dimensional random vector $\\mathbf{X} = (X_1, \\ldots, X_k)$ is said to have a Multivariate Normal (MVN) distribution if every linear combination of the $X_j$ has a Normal distribution. Similarly, the marginal distribution of each $X_j$ is Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4179c1b-153a-40a4-98b7-c04b149b7da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46aba849-ba8f-489f-ab5a-e7f01d247e73",
   "metadata": {},
   "source": [
    "<h4>Theorem 7.5.4</h4>\n",
    "\n",
    "If $(X_1, X_2, X_3)$ is MVN, then so is the subvector $(X_1, X_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7798c-6094-4417-9e9c-11c828b863b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7b8e4a-ea9b-43c9-b330-b102271b7e89",
   "metadata": {},
   "source": [
    "<h4>Theorem 7.5.5</h4>\n",
    "\n",
    "If $\\mathbf{X} = (X_1, \\ldots, X_n)$ and $\\mathbf{Y} = (Y_1, \\ldots, Y_m)$ are MVN random vectors with $\\mathbf{X}$ independent of $\\mathbf{Y}$, then the concatenated random vector $\\mathbf{W} = (X_1, \\ldots, X_n, Y_1, \\ldots, Y_m)$ is MVN.\n",
    "\n",
    "A MVN is fully specified by knowing the mean of each component, the varaince of each component, and the covariance or correlation between any two components. i.e., the parameters of an MVN random vector $(X_1, \\ldots, X_k)$ are the mean vector $(\\mu_1, \\ldots, \\mu_k)$, where $E(X_j) = \\mu_j$, and the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cb4c5-a5b1-4431-83b8-654c3d11c517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da87748-8880-4e59-ab33-68c21c0ae166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c08bbe-03cb-4899-b2b2-538e6660a81d",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9f84d-ec0b-4663-8118-ee9abfcacc4a",
   "metadata": {},
   "source": [
    "<h3>Covariance Exercises</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158a576-e9c7-41e0-81e9-f3929d64dd77",
   "metadata": {},
   "source": [
    "<h4>Exercise 39</h4>\n",
    "\n",
    "Two fair six-sided dice are rolled, one green and one orange, with outcomes $X$ and $Y$ respectively for the green and the orange.\n",
    "\n",
    "<b>Part A:</b>\n",
    "\n",
    "Compute the covariance of $X+Y$ and $X-Y$.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "$Cov(X+Y, X-Y) = Cov(X,X) - Cov(X,Y) + Cov(Y,X) - Cov(Y,Y) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083407ed-b9f7-469b-99f8-b8e862f0a631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20a1f928-ac13-4471-b404-c98dc4022d90",
   "metadata": {},
   "source": [
    "<h4>Exercise 41</h4>\n",
    "\n",
    "Let $X$ and $Y$ be standardized random variables with correlation $\\rho \\in (-1,1)$. Find $a,b,c,d$ (in terms of $\\rho$) such that $z = aX + bY$ and $W = cX + dY$ are uncorrelated but still standardized.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "$Cov(Z,W) = Cov(X, cX + dY) = Cov(X, cX) + Cov(X, dY) = c + d \\rho = 0$\n",
    "\n",
    "Also, $Var(W) = c^2 + d^2 + 2 cd \\rho = 1$. Solving for $c, d$ gives:\n",
    "\n",
    "$a=1, b=0, c = -\\rho / \\sqrt{1 - \\rho^2} = 1 \\sqrt{1-\\rho^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a9885-7cc3-43de-b035-8bfb0e0ca3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f11593-8ca5-437a-a91f-fc825a1691de",
   "metadata": {},
   "source": [
    "<h4>Exercise 52</h4>\n",
    "\n",
    "A drunken man wanders around randomly in a large space. At each step, he moves one unit of distance north, south, east, or west, with equal probabilities. Choose coordinates such that his initial position is $(0,0)$ and if he is at $(x,y)$ at some time, then one step later he is at $(x,y+1)$, $(x,y-1)$, $(x+1,y)$, or $(x-1,y)$. Let $(X_n, Y_n)$ and $R_n$ be his position and distance from the origin after n steps, respectively. Find $Cov(X_n, Y_n)$.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Write $X_n = \\sum_{i=1}^n z_i$ and $Y_n \\ sum_{j=1}^n w_j$, where $Z_i$ is $-1$ if his $i^{th}$ step is westward, $1$ if his $i^{th}$ step is eastward, and $0$ otherwise, and similarly for $W_j$. Then $Z_i$ is independent of $W_j$ for $i \\neq j$. But $Z_i + W_i$ are highly dependent. Exactly one of them is $0$ since he moves in one direction at a time. Then $Cov(Z_i, W_i) = E(Z_i W_i) - E(Z_i) E(W_i) = 0$, since $Z_i W_i$ is always $0$, and $Z_i$ and $W_i$ have mean $0$. So:\n",
    "\n",
    "$Cov(X_n, Y_n) = \\sum_{i,j} Cov(Z_i, W_j) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa035a-dcb6-494b-b79a-bf5b809f7719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f00c930-a56e-46b2-8437-3d8797c5ec00",
   "metadata": {},
   "source": [
    "<h4>Exercise 55</h4>\n",
    "\n",
    "Consider the following method for creating a bivariate Poisson (a joint distribution for two random variables such that both marginals are Poissons). Let $X=V+W$, $Y=V+Z$, where $V$, $W$, $Z$ are IID $Pois(\\lambda)$.\n",
    "\n",
    "<b>Part A:</b>\n",
    "\n",
    "Find $Cov(X,Y)$\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Using the properties of covariance, we have:\n",
    "\n",
    "$Cov(X,Y) = Cov(V,V) + Cov(V,Z) + Cov(W,V) + Cov(W,Z) = Var(V) = \\lambda$\n",
    "\n",
    "<b>Part B:</b>\n",
    "\n",
    "Are $X$ and $Y$ independent? Are they conditionally independent given $V$?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Since $X$ and $Y$ are correlated (with covariance $\\lambda \\gt 0$), they are not independent. Alternatively, note that $E(Y) = 2 \\lambda$ but $E(Y|X=0) = \\lambda$, since if $X=0$ occurs then $V=0$ occurs. But $X$ and $Y$ are conditionally independent given $V$, since the conditional joint PMF is:\n",
    "\n",
    "$P(X=x, Y=y|V=v) = P(W=x-v, Z=y-v|V=v) = P(W=x-v, Z=y-v) = P(W=x-v)P(Z=y-v) = P(X=x|V=v) P(Y=y|V=v)$\n",
    "\n",
    "This makes sense intuitively since if we observe that $V=v$, then $X$ and $Y$ are the independent random variables $W$ and $Z$, shifted by constant $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72808c-3626-4a2c-89a7-06f91eb8f57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5067012-b919-4d27-8778-176408f29164",
   "metadata": {},
   "source": [
    "<h3>Multinomial Exercises</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c7ae9b-ff87-4373-a710-a9ffb39510fa",
   "metadata": {},
   "source": [
    "<h4>Exercise 65</h4>\n",
    "\n",
    "Consider the birthdays of $100$ perople. Assume people's birthdays are independent, and the $365$ days of the year are equally likely. Find the covariance and correlation between how many of the people were born on January 1 and how many were born on January 2.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Let $X_j$ be the number of people born on January $j$. Then:\n",
    "\n",
    "$Cov(X_1, X_2) = - \\frac{100}{365^2}$\n",
    "\n",
    "using the result about covariances in a Multinomial. Since $X_j \\text{~} Bin \\left( 100, \\frac{1}{365} \\right)$, we then have:\n",
    "\n",
    "$Corr(X_1, X_2) = \\frac{Cov(X,Y)}{\\sqrt{Var(X) Var(Y)}} = \\frac{ 100/365^2 }{ 100 \\left( \\frac{1}{365} \\right) \\left( \\frac{364}{365} \\right) } = - \\frac{1}{364}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6e347-054b-4f6c-ab7a-743099559a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c33f15e0-84c6-4a20-9aa6-c9aa61496a92",
   "metadata": {},
   "source": [
    "<h4>Exercise 68</h4>\n",
    "\n",
    "Emails arrive in an inbox according to a Poisson process with rate \\lambda, independently. Let $X$, $Y$, $Z$ be the numbers of emails that arrive from 9am to noon, noon to 6pm, and 6pm to midnight.\n",
    "\n",
    "Part A:\n",
    "\n",
    "Find the joint PMF of $X$, $Y$, and $Z$.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Since $X \\text{~} Pois(3 \\lambda), Y \\text{~} Pois(6 \\lambda), Z \\text{~} Pois(6 \\lambda)$ independently, the joint PMF is:\n",
    "\n",
    "$P(X=x, Y=y, Z=z) = \\frac{ e^{-3 \\lambda}(3 \\lambda)^x }{x!} + \\frac{ e^{-6 \\lambda}(6 \\lambda)^y }{y!} + \\frac{ e^{-6 \\lambda}(6 \\lambda)^z }{z!}$\n",
    "\n",
    "Part B:\n",
    "\n",
    "Find the conditional PMF of $X+Y$ given that $X+Y+Z = 36$, and find $E(X+Y|X+Y+Z = 36)$ and $Var(X+Y|X + Y + Z = 36)$.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Conditional expectation and conditional variance given an event are defined in the same way as expectation and variance, using the conditional distribution given the event in place of the unconditional distribution.\n",
    "\n",
    "Let $W=X+Y$ and $T=X+Y+Z$. Using the story of the Multinomial and part B, we can merge the categories 9am to noon and noon to 6pm to getL\n",
    "\n",
    "$W|T = 36 \\text{~} Bin \\left( 36, \\frac{9}{15} \\right)$\n",
    "\n",
    "Therefore, $E(W|T=36) = 36 \\cdot \\frac{9}{15} = 21.6$ and $Var(W|T=36) = 36 \\cdot \\frac{9}{15} \\cdot \\frac{6}{15} = 8.64$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
