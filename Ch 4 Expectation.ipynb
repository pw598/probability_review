{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392f39a4-4024-4870-b229-9806e7aa4beb",
   "metadata": {},
   "source": [
    "# Chapter 4: Expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731cec3-895d-4afa-b924-b9cdee6ebfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source of the content is freely available online\n",
    "# https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view\n",
    "# https://projects.iq.harvard.edu/stat110/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f151bd7-2f8e-4510-a581-88d2174247fe",
   "metadata": {},
   "source": [
    "Story 4.3.1 (Geometric Distribution)\n",
    "\n",
    "Consider a sequence of independent Bernoulli trials, each with the same success probability, with trials performed until a success occurs. Let $X$ be the number of failures before the first successful trial. Then, $X$ has the Geometric distribution with parameter $p$.\n",
    "\n",
    "Imagine the Bernoulli trials as a string of zeroes (failures) ending in a single success ($1$). Each $0$ has probability $p$, so a string of $k$ failures followed by one success has probability $q^kp$.\n",
    "\n",
    "The PDF of the Geometric is $P(X=k) = q^kp$ for $k = 0,1,2,\\ldots,$ where $q=1-p$. This is a valid PMF because, summing a geometric series, we have:\n",
    "\n",
    "$\\sum_{k=0}^{\\infty} = p \\sum_{k=0}^{\\infty} q^k = p \\cdot \\frac{1}{1-q} = 1$\n",
    "\n",
    "There are different conventions for the definition of the Geometric distribution. Some sources define it as the total number of trials, including the success. In this book, the Geometric distribution excludes the success, and the FirstSuccess distribution includes the success. If $X \\text{~} Geom(p)$, then $X+1 \\text{~} FS(p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b01ac-3dbf-4c62-ad7a-0ba84cf9042f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7b841e-3786-44d4-843b-4bf50db7885e",
   "metadata": {},
   "source": [
    "<h4>Example 4.3.7 (First Success Expectation)</h4>\n",
    "\n",
    "$E(Y) = E(X+1) = \\frac{q}{p} + 1 = \\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7f0c0-28d9-41aa-b315-a667adbee1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e303a5d0-b3a7-4b4e-b297-259c07fe9719",
   "metadata": {},
   "source": [
    "<h4>Story 4.3.8 (Negative Binomial Distribution)</h4>\n",
    "\n",
    "In a sequence of independent Bernoulli trials with success probability $p$, if $X$ is the number of failures before the $r^{th}$ success, then $X$ is said to have the Negative Binomial distribution with parameters $r$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28848ace-6257-4b3c-a6e0-39f49f843163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee89bd20-79e8-4a61-a57b-258ebe7559d4",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.3.9</h4>\n",
    "\n",
    "If $X \\text{~} NBin(r,p)$, then the PMF of $X$ is:\n",
    "\n",
    "$P(X=n) = \\binom{n+r+1}{r-1} p^r q^n$\n",
    "\n",
    "for $n = 0,1,2,\\ldots$, where $q=1-p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6ceff-17d2-45fb-a274-db23d97f5389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9cd2eb5-4093-408e-82a5-9951c24b04f7",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.3.10</h4>\n",
    "\n",
    "Let $X \\text{~} Bin(r,p)$, viewed as the number of failures before the $r^{th}$ success in a sequence of independent Bernoulli trials with success probability $p$. Then we can write $X=X_1 + \\ldots + X_r$, where the $X$ are IID Geom(p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88983145-357b-4346-a61b-977a4d58ea61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81de0c97-68cf-408e-9b05-3c008d90ebfe",
   "metadata": {},
   "source": [
    "<h4>Example 4.3.11 (Negative Binomial Expectation)</h4>\n",
    "\n",
    "By linearity:\n",
    "\n",
    "$E(X) = E(X_1) + \\ldots + E(X_r) = r \\cdot \\frac{q}{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c840809-8750-4f6e-bb81-1e4bdd82b034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7088666-d55a-4209-be31-281b79212ac0",
   "metadata": {},
   "source": [
    "<h4>Example 4.3.12 (Coupon Collector)</h4>\n",
    "\n",
    "There are $n$ types of toys, which you are collecting toys, the toy types are random (e.g., included in cereal boxes); it is equally likely to be any of the $n$ types. What is the expected number of toys needed until you have a complete set?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Our strategy will be to break up N into a sum of simpler random variables so that we can apply linearity.\n",
    "\n",
    "$N = N_1 + N_2 + \\ldots + N_n$\n",
    "\n",
    "where $N_1$ is the number of toys until the first toy type you haven't seen before, $N_2$ is the additional number of toys until the second toy you haven't seen before, and so forth.\n",
    "\n",
    "**img pg 162**\n",
    "\n",
    "By the story of the FS distribution, $N_2 \\text{~} FS((n-1)/n))$. After collecting the first toy type, there is a $1/n$ chance of getting the same toy you already had (i.e. failure) and a $(n-1)/n$ chance you'll get something else (success). $N_3$ is distributed $FS((n-2/n))$.\n",
    "\n",
    "$E(N) = E(N_1) + E(N_2) + \\ldots + E(N_n) = n \\sum_{j=1}^n \\frac{1}{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed37749-d450-4b8b-aaf7-669de16cdac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33767f1d-0a0c-4a70-91b9-33311bd02d5f",
   "metadata": {},
   "source": [
    "<h4>Example 4.5 Law of the Unconscious Statistician (LOTUS)</h4>\n",
    "\n",
    "$E(g(X))$ does not equal $g(E(X))$ in general if $g$ is not linear. But it is possible to find $E(g(X))$ directly using the distribution of $X$, without having to find the distribution of $g(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0249f94-5597-4746-831e-b0793788e2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e4791f7-5c24-404d-a8b2-7b7acced1dcd",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.5.1 (LOTUS)</h4>\n",
    "\n",
    "If $X$ is a discrete random variable and $g$ is a function from $\\mathbb{R}$ to $\\mathbb{R}$, \n",
    "\n",
    "$E(g(X)) = \\sum_x g(x) P(X=x)$\n",
    "\n",
    "This means that we can get the expected value of $g(X)$ knowing only $P(X=x)$, the PMF of $X$. We don't need to known the PMF of $g(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587278f2-bb6d-4d0a-926d-cf519541a2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "722aa7bc-1a2d-4de2-894c-b1b93ead4dd4",
   "metadata": {},
   "source": [
    "<h3>4.6 Variance</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42db511-e108-4098-bc5f-d772f5e888e7",
   "metadata": {},
   "source": [
    "<h4>Definition 4.61 (Variance and Standard Deviation)</h4>\n",
    "\n",
    "The variance of a random variable is:\n",
    "\n",
    "$Var(X) = E(X - E(X))^2$\n",
    "\n",
    "The square root is the standard deviation.\n",
    "\n",
    "$SD(X) = \\sqrt{Var(X)}$\n",
    "\n",
    "An equivalent equation for variance is:\n",
    "\n",
    "$Var(X) = E(X)^2 - E(X)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03cee6-1a06-488e-b5a8-55ca882e2738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7024f3d3-01c9-4d19-8f03-d5e5346b3924",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.6.2 (Properties of Variance)</h4>\n",
    "\n",
    "- $Var(X+c) = Var(X)$ for any constant $c$. $c$ shifts the distribution of $X$ to the left or right, but does not affect the level of spread.\n",
    "\n",
    "- $Var(cX) = c^2 Var(X)$ for any constant $c$.\n",
    "\n",
    "- If $X$ and $Y$ are independent, then $Var(X+Y) = Var(X) + Var(Y)$\n",
    "\n",
    "- $Var(X)=0$; the only random variables which have zero variables are degenerate random variables, i.e., constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642587a-e32e-4e48-8c98-eaa755edc788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d517cd49-4630-4f7c-bcad-a6b3a175e54f",
   "metadata": {},
   "source": [
    "<h3>Poisson Distribution</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcb890-021d-4bde-bcbc-d0c3054f2934",
   "metadata": {},
   "source": [
    "<h4>Definition 4.7.1 (Poisson Distribution)</h4>\n",
    "\n",
    "A random variable has the Poisson distribution with parmeter $\\lambda$, where $\\lambda \\gt 0$, if the PMF is:\n",
    "\n",
    "$P(X=k) = \\frac{ e^{-\\lambda} \\lambda^k }{k!}, k = 0,1,2,\\ldots$\n",
    "\n",
    "This is a valid PMF because of the Taylor series:\n",
    "\n",
    "$\\sum_{k=0}^{\\infty} = e^{\\lambda}$\n",
    "\n",
    "The mean and variance of the Poisson distribution are both equal to $\\lambda$.\n",
    "\n",
    "**get Poisson content from notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4dc16-25fe-4652-acf7-a2665dbbdfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba234d2-5969-4690-921c-cdad0e3082fe",
   "metadata": {},
   "source": [
    "<h4>Example 4.7.5 (Birthday Problem Continued)</h4>\n",
    "\n",
    "If we have $m$ people and make the usual assumptions, then each pair of people has $p=\\frac{1}{365}$ of having the same birthday, and there are $\\binom{m}{2}$ pairs. By the Poisson paradigm, the distribution of the number $X$ of birthday matches is approximately $Pois(\\lambda)$, where $\\lambda = \\binom{m}{2} \\frac{1}{365}$. Then, the probability of at least one match is:\n",
    "\n",
    "$P(X \\le 1) = 1 - P(X=0) \\approx 1-e^{-\\lambda}$\n",
    "\n",
    "For $m=23$, $\\lambda = \\frac{253}{365}$ and $1-e^{\\lambda} \\approx 0.500002$ (which agrees with our earlier finding that we needd $23$ people to have a $50\\%$ chance of a match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6fb0d-77f5-4955-ae63-b54df6689031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29ea2c7c-9087-45fd-914b-87b7e5020219",
   "metadata": {},
   "source": [
    "<h4>Example 4.7.6 (Birthday Problem)</h4>\n",
    "\n",
    "What if we want to find the number of people required in oorder to have a $50/50$ chance that two people have birthdays within $1$ day of each other (i.e., same day or one day behind or ahead).\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "The probability that any two people have birthdays within one day of each other is $3/365$ (choose a birthday for the first person, and then the second person has to be born on that day, the day before, or the day after). There are $\\binom{m}{2}$ possible pairs, so the number of within-one-day matches is approximately $Pois(\\lambda)$ where:\n",
    "\n",
    "$\\lambda = \\binom{m}{2} \\frac{3}{365}$\n",
    "\n",
    "A calculation similar to the one above tells us that we need m=14 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257e136-5ada-4447-a19c-6b4c246171b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac12b46-45f5-4fca-980c-801b34b116c1",
   "metadata": {},
   "source": [
    "<h4>Example 4.7.7 (Birth-Minute and Birth-Hour)</h4>\n",
    "\n",
    "There are $1600$ sophomores at a certain college. Throughout this example, make the usual assumptions as in the birthday problem.\n",
    "\n",
    "<b>Part A:</b>\n",
    "\n",
    "Find a Poisson approximation for the probability that there are the sophomores born not only on the same day, but at the same hour as well (but not necessarily the same year).\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "This is the birthday problem with $c = 365 \\cdot 24 \\cdot 60 = 525,600$ categories rather than 365.\n",
    "\n",
    "Creating an indicator random variable for each pair of sophomores, by linearity, the expected number of pairs born on the same day-hour-minute combination is:\n",
    "\n",
    "$\\lambda_1 = \\binom{n}{2} \\frac{1}{c}$\n",
    "\n",
    "By Poisson approximation, the probability of at least one match is approximately:\n",
    "\n",
    "$1 - exp(-\\lambda_1) \\approx 0.9122$\n",
    "\n",
    "\n",
    "<b>Part B:</b>\n",
    "\n",
    "With assumptions as in part A, what is the probability that there are 4 sophommores born on the same day and hour?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Now there are $b = 365 \\cdot 24 = 8760$ categories.\n",
    "\n",
    "Create an indicator for each set of $4$ sophomores. By linearity, the expected number of sets of $4$ sophomores born on the same day-hour is:\n",
    "\n",
    "$\\lambda_2 = \\binom{n}{4} \\frac{1}{b^3}$\n",
    "\n",
    "Poisson approximation gives that the desired probability is approximately:\n",
    "\n",
    "$a - exp(-\\lambda_2) \\approx 0.333$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7ad37-a05e-40cf-ba84-941fc712af71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df98aa51-d65a-47c8-9f57-51c06635e47d",
   "metadata": {},
   "source": [
    "<h4>Connections Between Poisson and Binomial</h4>\n",
    "\n",
    "The Poisson and Binomial distributions are closely connected and their relationship is parallel to that of the Binomial and Hypergeometric distributions. We can get from the Poisson to the Binomial by conditioning and we can get from the Binomial to the Poisson by taking a limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79535cb3-a7f5-4fef-b86a-a8e5255e8c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be345046-feda-41f1-8de1-6b1189a0339f",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.8.1</h4>\n",
    "\n",
    "If $X \\text{~} Pois(\\lambda)$, $Y \\text{~} Pois(\\lambda_2)$, and $X$ is independent of $Y$, then $X+Y \\text{~} Pois(\\lambda_1 + \\lambda_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e98e66-5124-4e8f-9ab6-fd847564d7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a915a2e6-49e6-4ae2-bf14-c45784f1d94e",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.8.2 (Poisson Given a Sum of Poissons)</h4>\n",
    "\n",
    "If $X \\text{~} Pois(\\lambda)$, $Y \\text{~} Pois(\\lambda_2)$, and $X$ is idependent of $Y$, then the conditional distribution of $X$ given $X+Y=n$ is $Bin(n, (\\lambda_1 / \\lambda_2))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f16af-139b-4918-be42-62f97027975c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3db0320b-12c4-452c-8d60-a118550e3a9a",
   "metadata": {},
   "source": [
    "<h4>Definition 4.9.4 (Binary Entropy Function)</h4>\n",
    "\n",
    "For $0 \\lt p \\lt 1$, the binary entropy function $H$ is given by:\n",
    "\n",
    "$H(p) = -p ~log_2p - (1-p) ~log_2(1-p)$\n",
    "\n",
    "We define $H(0) = H(1) = 0$.\n",
    "\n",
    "The interpretation in information theory is that it is a measure of how much information we get from observing a $Bern(p)$ random variable. $H(1/2)=1$ means that a fair coin flip provides $1$ bit of information, while $H(1)=0$ says that with a coin that always lands heads, there is no information gained from being told the result of the flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b9b3a-a67e-4869-a3bd-4214d86ec5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96f9ecb5-67df-43af-a5c6-ca6ba5167246",
   "metadata": {},
   "source": [
    "<h4>Theorem 4.9.5 (Shannon)</h4>\n",
    "\n",
    "Consider a channel where each transmitted bit gets flipped with probability $p$, independently. Let $0 \\lt p \\lt \\frac{1}{2}$ and $\\varepsilon \\gt 0$. There exists a code with rate at least $1-H(p) - \\varepsilon$ that can be decoded with probability of error less than $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bb634-55c6-40f3-bc04-de66d4838236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0a56a-f013-43c2-8d92-8074ffd42529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce8f4e1-2d75-4a9f-a97c-e1d4937f3d9f",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc81e14-02e0-4f1d-b9bc-3ae1f00f2308",
   "metadata": {},
   "source": [
    "<h4>Named Distribution Exercises</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f1845-73c1-49ee-a45a-16b36b6322c1",
   "metadata": {},
   "source": [
    "Raindrops are falling at an average rate of $20$ drops per square inch per minute. What would be a reasonable distribution for the number of raindrops hitting a particular region measuring $5$ inches squared in $t$ minutes? Compute the probability that the region has no rain drops in a given 3-second interval.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "A reasonable choice of distribution is $Pois(\\lambda t)$, where $\\lambda = 20 \\cdot 5 = 100$ (the average number of raindrops per minute hitting the region). Assuming this distribution:\n",
    "\n",
    "$P(\\text{no raindrops in 1/20 of a minute}) = e^{-100/20} (100/20)^0 / 0! \\approx e^{-5} \\approx 0.0067$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cdafae-db30-404d-9b09-e0ed723b685f",
   "metadata": {},
   "source": [
    "<h4>Exercise 22</h4>\n",
    "\n",
    "Alice and Bob have just met, and wonder whether they have a mutual friend. Each has $50$ friends, out of $1000$ other people who live in their town. They think it's unlikely that they have a friend in common.\n",
    "\n",
    "Assume that Alice's 50 friends are a random sample of the $1000$ people, and similarly for Bob. Assume independence.\n",
    "\n",
    "<b>Part A:</b>\n",
    "\n",
    "Compute the expected number of mutual friends.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Let $I_j$ be the indicator random variable for the $j^{th}$ person being a mutual friend. Then:\n",
    "\n",
    "$E \\left( \\sum_{j=1}^{1000} I_j \\right) = 1000 E(I_1) = 1000 P(I_1=1) = 1000 \\cdot \\left( \\frac{5}{100} \\right)^2 = 2.5$\n",
    "\n",
    "\n",
    "<b>Part B:</b>\n",
    "\n",
    "Let $X$ be the number of mutual friends they have. Find the PMF of $X$.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Condition on who Alice's friends are, then count the number of ways that Bob can be friends with exactly $k$ of them. This gives:\n",
    "\n",
    "$P(X=k) = \\frac{ \\binom{50}{k} \\binom{950}{50-k} }{ \\binom{1000}{50} }$\n",
    "\n",
    "i.e., the Hypergeometric distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cecd10-73ae-41c2-bbf5-9295ea4331eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626b0a83-9008-4dc7-9d18-34311af22f14",
   "metadata": {},
   "source": [
    "<h4>Exercise 24</h4>\n",
    "\n",
    "Calvin and Hobbes play a match consisting of a series of games, where Calvin has a probability p of winning each game independently. The first player to win two games more than his opponent wins the match. Find the expected number of games played.\n",
    "\n",
    "Hint: Consider the first two games as a pair, then the next two as a pair, etc.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Think of the first 2 games, the 3rd and 4th, the 5th and 6th, etc., as mini-matches. The match ends after the first mini-match that isn't a tie. The probability of a mini-match not being a tie is $p^2 + q^2$, so the number of mini-matches needed is $1$ plus a $Geom(p^2 + q^2)$ random variable. Thus, the expected number of games is $\\frac{2}{(p^2 + q^2)}$.\n",
    "\n",
    "For $p=0$ or $p=1$, this reduces to $2$. The expected number of games is maximized when $p=\\frac{1}{2}$, which makes sense intuitively. Also, it makes sense that the result is symmetric in $p$ and $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e46f0d-d12e-44a9-b764-a5a91dc35f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4aeb647-4230-4f46-a474-e43ebac3552b",
   "metadata": {},
   "source": [
    "<h4>Exercise 26</h4>\n",
    "\n",
    "Let $X$ and $Y$ be $Pois(\\lambda)$ random variables, and $T=X+Y$. Suppose that $X$ and $Y$ are not independent, and in fact $X=Y$. Prove or disprove the claim that $T \\text{~} Pois(2 \\lambda)$ in this scenario.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "The random variable $T=2X$ is not Poisson, and can only take even values $0,2,4,6, \\ldots$, whereas any Poisson random variable has positive probabilities of being $0,1,2,3, \\ldots$. Alternatively, we can compute the PMF of $2X$, or note that $Var(2X) = 4 \\lambda \\neq 2 \\lambda = E(2X)$, whereas for any Poisson random variable the variance equals the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb3915-ba67-4e76-92d5-991e5cd6b8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7041269c-128a-4419-94ad-8e0840c6096f",
   "metadata": {},
   "source": [
    "<h4>Exercise 33</h4>\n",
    "\n",
    "A total of $20$ bags of gummy bears are randomly distributed to $20$ students. Each bag is obtained by a random student, and the outcomes are independent. Find the average number of gummy bears that the first $3$ students get in total, and find the average number of students who get at least one bag.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Let $X_j$ be the number of bags of gummy bears that the $j^{th}$ student gets, and let $I_j$ be the indicator of $X_j \\gt 1$. Then $X_j \\text{~} Bin(20,\\frac{1}{20})$, so $E(X_j) \\ge 1$. So $E(X_1 + X_2 + X_3) = 3$ by linearity.\n",
    "\n",
    "The average number of students who get at least one bag is:\n",
    "\n",
    "$E(I_1 + \\ldots + I_{20}) = 20 E(I_1) = 20 P(I_1 = 1) = 20 \\left( 1 - \\left( \\frac{19}{20} \\right)^{20} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce41ba3-9acc-47ad-a287-09512b6a33ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0b099-bea2-45b7-b4c6-47453ac50c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new section name?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd700d69-216e-4001-9df6-661ae3d27665",
   "metadata": {},
   "source": [
    "<h4>Exercise 52</h4>\n",
    "\n",
    "An urn contains red, green, and blue balls. Balls are chosen randomly with replacement. Let $r,g,b$ be the probabilities of drawing a red, green, or blue ball respectively ($r+g+b=1$).\n",
    "\n",
    "<b>Part A:</b>\n",
    "\n",
    "Find the expected number of balls chosen before obtaining the first red ball, not including the red ball itself.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "The distribution is $Geom(r)$, so the expected value is $\\frac{1-r}{r}$.\n",
    "\n",
    "\n",
    "<b>Part B:</b>\n",
    "\n",
    "Find the expected number of different colors of balls obtained before getting the first red ball.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Use indicator random variables: let $I_1$ be $1$ if green is obtained before red, and 0 otherwise, and define $I_2$ similarly for blue. Then:\n",
    "\n",
    "$E(I_1) = P(\\text{green before red}) = \\frac{g}{g+r}$\n",
    "\n",
    "since 'green before red' means that the first nonblue ball is green. Similarly, $E(I_2) = b/(b+r)$, so the expected number of colors obtained before getting red is:\n",
    "\n",
    "$E(I_1 + I_2) = \\frac{g}{g+r} + \\frac{b}{b+r}$\n",
    "\n",
    "\n",
    "<b>Part C:</b>\n",
    "\n",
    "Find the probability that at least two of n balls drawn are red, given that at least 1 is red.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "By definition of conditional probability:\n",
    "\n",
    "$P(\\text{at least 2 red | at least 1 red}) = \\frac{P \\text{at least 2 red}}{P \\text{at least 1 red}} = \n",
    "\\frac{ 1-(1-r)^n - nr(1-r)^{n-1} }{ 1-(1-r)^n }$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
