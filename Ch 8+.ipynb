{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bcd222c-2e17-4dac-bb90-a396581cf103",
   "metadata": {},
   "source": [
    "# Chapter 8: Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a313d-30eb-4d3e-90f6-b1c4b34e9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source of the content is freely available online\n",
    "# https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view\n",
    "# https://projects.iq.harvard.edu/stat110/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035c097-82e6-4ee4-ba6c-6b28417244da",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Definition 8.3.1</h4>\n",
    "\n",
    "A random variable $X$ is said to have the Beta distribution with parameters $a$ and $b$, where $a \\gt 0$ and $b \\gt 0$, if its PDF is:\n",
    "\n",
    "$f(x) \\frac{1}{\\beta(a,b)} x^{a-1} (1-x)^{b-1}, 0 \\lt x \\lt 1$\n",
    "\n",
    "where the constant $\\beta(a,b)$ is chosen to make the PDF integrate to $1$.\n",
    "\n",
    "The $Beta(1,1)$ and $Unif(0,1)$ distributions are the same. By varying the values of $a$ and $b$, we get PDFs with a variety of shapes.\n",
    "\n",
    "<img src=\"img/fig 8-4.png\" style=\"height: 400px; width:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad7e65-57b0-42f4-b448-bd5c522667b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753afa28-5414-4a0c-b9dc-ca1943a93b6b",
   "metadata": {},
   "source": [
    "<h4>8.4 Gamma</h4>\n",
    "\n",
    "The Gamma distribution is a generalization of the Exponential distribution. While an Exponential random variable represents the waiting time for the first success under conditions of memorylessness, we shall see that a Gamma random variable represents the total waiting time for multiple successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86631266-a1a0-4bb5-b003-f55845449743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e0a735-c94b-4ce0-a707-5ed4223cc766",
   "metadata": {},
   "source": [
    "<h4>Definition 8.4.1 (Gamma Function)</h4>\n",
    "\n",
    "The gamma function $\\Gamma$ is defined by:\n",
    "\n",
    "$\\Gamma(a) = \\int_0^\\infty x^a e^{-x} ~\\frac{dx}{x}$\n",
    "\n",
    "for real numbers $a \\gt 0$.\n",
    "\n",
    "$\\Gamma (n) = (n-1)!$ if $n$ is a positive integer. This can be proved by starting with $n=1$ and using the recursive relation $\\Gamma(a+1) = a \\Gamma (a)$.\n",
    "\n",
    "If we divide both sides of the above definition by $\\Gamma(a)$, we have:\n",
    "\n",
    "$1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(a)} x^a e^{-x} \\frac{dx}{x}$\n",
    "\n",
    "We say that $X$ has the Gamma distribution with parameters $a$ and $1$, denoted $X \\text{~} Gamma(a,1)$, if its PDF is:\n",
    "\n",
    "$f_X(x) = \\frac{1}{\\Gamma(a)} x^a e^{-x} \\frac{1}{x}, x \\gt 0$\n",
    "\n",
    "We obtain the general Gamma distribution by a scale transformation. If $X \\text{~} Gamma(a,1) and \\lambda \\gt 0$, then the distribution of $Y = X / \\lambda$ is called the $Gamma(a,\\lambda)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843249d-f469-409b-8039-11e5ad33c8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d6deea-828c-411f-abce-96cb6addd659",
   "metadata": {},
   "source": [
    "<h4>Definition 8.4.2 (Gamma Distribution)</h4>\n",
    "\n",
    "A random variable is said to have the Gamma distribution with parameters $a$ and $\\lambda$, where $a \\gt 0$ and $\\lambda \\gt 0$, if its PDF is:\n",
    "\n",
    "$f(y) = \\frac{1}{\\Gamma (a)} (\\lambda y)^a e^{-\\lambda y} \\frac{1}{y}, for y \\gt 0$.\n",
    "\n",
    "Taking $a=1$, the $Gamma(1, \\lambda)$ PDF is $f(y) = \\lambda e^{- \\lambda y}$ for $y \\gt 0$, so the $Gamma(1, \\lambda)$ and $Expo(\\lambda)$ distributions are the same.\n",
    "\n",
    "<img src=\"img/fig 8-7.png\" style=\"height: 400px; width:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb7b8c-b9d8-41c5-a16b-076c51653e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047b99ff-3b9b-4974-985d-c03b219f5934",
   "metadata": {},
   "source": [
    "<h4>Theorem 8.4.3</h4>\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ be IID $Expo(\\lambda)$. Then:\n",
    "\n",
    "$X_1 + \\ldots + X_n \\text{~} Gamma(n, \\lambda)$\n",
    "\n",
    "\n",
    "Theorem 8.4.3 allows us to connect the Gamma distribution to the story of the Poisson process. In a Poisson process of rate $\\lambda$, the interarrival times are IID $Expo(\\lambda)$ random variables. The total waiting time $T_n$ for the $n^{th}$ arrival is the sum of the first $n$ interarrival times. e.g., $T_3$ is the sum of the interarrival times $X_1, X_2, X_3$. Therefore, $T \\text{~} Gamma(n,\\lambda)$ and the interarrival times in a Poisson process are Exponential random variables, where the raw arrival times are Gamma random variables.\n",
    "\n",
    "<img src=\"img/fig 8-8.png\" style=\"height: 200px; width:auto;\">\n",
    "\n",
    "In a Poisson process of rate $\\lambda$, the total waiting time for $n$ arrivals is distributed $Gamma(n,\\lambda)$. Thus, the Gamma is the continuous analog of the Negative Binomial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
