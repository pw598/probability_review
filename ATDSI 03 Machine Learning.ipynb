{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f9fb36-9bb4-4cdc-8ff4-68cfb4f0baa9",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b5aa3-6798-48fc-a845-81207f4134ca",
   "metadata": {},
   "source": [
    "<h2>End-to-End ML Workflow</h2>\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li>Clarify the Problem and Constraints</li>\n",
    "    <ul>\n",
    "        <li>Dependent Variable</li>\n",
    "        <li>Current State, Baseline Performance</li>\n",
    "        <li>Is ML Needed?</li>\n",
    "        <li>Legal/Ethical/Regulatory Concerns</li>\n",
    "        <li>Impact of Incorrect Predictions</li>\n",
    "        <li>Technical Requirements</li>\n",
    "        <li>Latency, throughput, deployment</li>\n",
    "    </ul>\n",
    "\n",
    "<li>Establish Metrics</li>\n",
    "<li>Constraints on recall, etc.</li>\n",
    "\n",
    "<li>Understand Data Sources</li>\n",
    "\n",
    "<li>Explore the Data</li>\n",
    "\n",
    "<li>Clean the Data</li>\n",
    "\n",
    "<li>Feature Engineering</li>\n",
    "    <ul>\t\t\t\n",
    "        <li>Filtering</li>\n",
    "        <li>Transformations</li>\n",
    "        <li>Binning</li>\n",
    "        <li>Dimension Reduction</li>\n",
    "        <li>One-Hot Encoding</li>\n",
    "        <li>Hashing</li>\n",
    "        <li>Stemming, Lemmatization, BOW, N-Grams, Word Embeddings</li>\n",
    "    </ul>\n",
    "\n",
    "<li>Model Selection</li>\n",
    "<li>Model Training and Evaluation</li>\n",
    "<li>Deployment</li>\n",
    "<li>Iterate</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd04e8-2a7d-4923-a9d9-ecc690f831d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd36a0-975c-45c0-87c8-b02c0718d008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cee10df-b2b8-4a9e-8761-1dd4b3baf680",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35063ab6-2af5-406c-af46-12ffa55d3a17",
   "metadata": {},
   "source": [
    "<h4>Question 1</h4>\n",
    "\n",
    "Say you are building a binary classifier to an unbalanced dataset (99% vs. 1%). How should you handle this situation?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "- Check whether you can get more data\n",
    "- Look at appropriate performance metrics: precision, recall, F1 score, ROC curve\n",
    "- Resample either by oversampling the rare samples or undersampling the abundant ones (bootstrapping)\n",
    "- Consider generating synthetic samples such as with SMOTE\n",
    "- Consider adjusting the probability threshold to something other than $0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea1396-262c-400c-9303-8f5499a8a8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f9dc478-59f9-4bd6-bbd9-ce5c6edd4153",
   "metadata": {},
   "source": [
    "<h4>Question 2</h4>\n",
    "\n",
    "What differences would you expect in a model that minimizes squared error vs. a model that minimizes absolute error?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Errors are squared before being averaged in MSE, meaning there is a relatively high weight given to large errors. Outliers affect MSE more than MAE; MAE is more robust to outliers.\n",
    "\n",
    "The gradient is more straightforward to calculate with MSE than MAE, which requires linear programming (?) to compute the gradient.\n",
    "\n",
    "Therefore, if the model needs to be computationally easier to train or doesn't need to be robust to outliers, then MSE should be used. Otherwise, MAE is a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab00c2-22a0-4c77-a73c-3b4919379125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dff2a29a-4860-47f5-8259-727865e2c250",
   "metadata": {},
   "source": [
    "<h4>Question 3</h4>\n",
    "\n",
    "When performing K-means clustering, how do you choose k?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "The elbow method is one alternative. The first few clusters will usually explain a lot of the variation in the data, but past a certain point, the amount of information added is diminishing. Looking at a graph of explained variation by number of clusters, there should be a sharp change in the y-axis at some level of $k$. The explained variation is quantified by the within-cluster sum of squared errors.\n",
    "\n",
    "The silhouette score is another method, which aims to measure how similar points are in its cluster compared to other clusters. It looks at:\n",
    "\n",
    "$\\frac{(x-y)}{max(x,y)}$\n",
    "\n",
    "$x$ is the mean distance to the examples of the nearest cluster, and $y$ is the mean distance to other examples in the same cluster, and y is the mean distance to other examples in the same cluster. The coefficient varies between $-1$ and $1$ for any given data point. A value of $1$ implies that the point is in the right cluster.\n",
    "\n",
    "Another alternative is to rely on business intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292b2b-1400-465a-8412-caf701e23db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebc1795f-ec37-48a0-8c12-57815d92d453",
   "metadata": {},
   "source": [
    "<h4>Question 4</h4>\n",
    "\n",
    "How can you make your models more robust to outliers?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "- Add regularization\n",
    "- Try different models (e.g., tree-based models are less affected than linear models)\n",
    "- \"Winsorize\" the data: cap the data at arbitrary thresholds. e.g., at $90\\%$ winsorization, we take the top and bottom $5\\%$ of values and set them to the values of the $95^{th}$ and $5^{th}$ percentile respectively.\n",
    "- Transform the data: e.g., do a log transformation when the response variable follows an exponential or right-skewed distribution\n",
    "- Change the error metric to be more robust to outliers (like MAE or Huber loss)\n",
    "- Remove outliers: if sure they are not worth incorporating into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17618b54-a49d-4a9e-b142-a6d22dd8fafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65579ef0-aa20-457b-9e67-bf7af5ba6ff8",
   "metadata": {},
   "source": [
    "<h4>Question 5</h4>\n",
    "\n",
    "In a multiple linear regression analysis, you have reason to believe that several of the predictors are correlated. How would the results of the regression be affected, and how would you deal with this problem?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "There will be two primary problems:\n",
    "\n",
    "- The coefficient estimates and signs will vary dramatically, depending on what particular variables are included in the model. The confidence intervals may include zero, bringing about uncertainty of direction.\n",
    "- The p-values will be misleading.\n",
    "\n",
    "This can be dealt with by removing or combining correlated predictors. It is possible to use interaction terms. Additionally, you could 1) center the data and 2) try to obtain a larger size of sample (thereby giving narrower confidence intervals). Lastly, you could apply regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecc79d-603f-4a7a-abcc-45a3585f017b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9818c761-f0fd-42fd-af07-500667d1025f",
   "metadata": {},
   "source": [
    "<h4>Question 6</h4>\n",
    "\n",
    "Describe the motivation behind random forests. What are two ways they improve upon individual decision trees?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Random forests are used because individual decision trees are prone to overfitting. There are a few ways in which they allow for stronger out-of-sample prediction than individual decision trees:\n",
    "\n",
    "- As in other ensemble models, bootstrap aggregating (bagging) will lead to a model yielding more consistent results, as it leads to diversity in training data for each tree.\n",
    "- Using $m \\lt p$ features (as the algorithm does) at each split helps to de-correlate the decision trees.\n",
    "- Easy to implement and fast to run\n",
    "- They produce interpretable feature importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c94625-1d4f-48f9-8786-3b6844ea71c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a036585c-3703-412e-b523-c9793953888c",
   "metadata": {},
   "source": [
    "<h4>Question 7</h4>\n",
    "\n",
    "<p>We want to predict the likelihood of transactions being fraudulent, however the data has many rows with missing values in various columns. How would you deal with this?</p>\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "<ol>\n",
    "<li>Step 1: Clarify the missing data</li>\n",
    "    <ul>\n",
    "        <li>Is the number of missing values uniform by feature?</li>\n",
    "        <li>Are the missing values numerical or categorical?</li>\n",
    "        <li>How many features are missing data?</li>\n",
    "        <li>Is there a pattern to waht's missing?</li>\n",
    "    </ul>\n",
    "\n",
    "<li>Step 2: Establish a baseline</li>\n",
    "    <ul>\n",
    "        <li>A good answer considers that the missing data may not be a problem. e.g., if it's for transactions that were almost never fraudulent, or features that are unimportant like name.</li>\n",
    "        <li>Can a baseline model be built that meets the business goals, without having to deal with the missing data?</li>\n",
    "    </ul>\n",
    "\n",
    "<li>Step 3: Impute missing data</li>\n",
    "    <ul>\n",
    "        <li>Mean, median, reference category</li>\n",
    "        <li>Nearest neighbors methods</li>\n",
    "    </ul>\n",
    "\n",
    "<li>Step 4: Check performance with imputed data</li>\n",
    "    <ul>\n",
    "        <li>Does performance increase by including the imputed data, as measured by cross-validation?</li>\n",
    "    </ul>\n",
    "\n",
    "<li>Step 5: Other Approaches</li>\n",
    "    <ul>\n",
    "        <li>e.g., using a third-party dataset to fill in gaps</li>\n",
    "    </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebff8-7601-432f-a564-ff22389d995a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a61502-a13e-4d49-b14b-f4ca1bfbe440",
   "metadata": {},
   "source": [
    "<h4>Question 8</h4>\n",
    "\n",
    "Say you are running a simple logistic regression to solve a problem but find the results to be unsatisfactory. What are some ways you might improve your model, or what models might you look into instead?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "- Normalize features: such that particular weights do not dominate within the model\n",
    "- Add additional features\n",
    "- Address outliers\n",
    "- Selecting features (reducing noise)\n",
    "- Hyperparameter tuning (e.g. regularization) with cross-validation\n",
    "- Nonlinear models (logistic regression produces linear decision boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb5834-a55c-403c-98aa-90ea3a6a5b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1e5b49-f474-416d-823a-98431ab8c5a9",
   "metadata": {},
   "source": [
    "<h4>Question 9</h4>\n",
    "\n",
    "Say you are running a linear regression for a dataset but you accidentally duplicated every data point. What happens to your $\\beta$ coefficient?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "It remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30964a6-6632-4d30-9323-c465e81c452a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5a47a7-c2d6-4bcd-9486-f96bd97b2214",
   "metadata": {},
   "source": [
    "<h4>Question 10</h4>\n",
    "\n",
    "Compare and contrast gradient boosting and random forests.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Both use ensembles of decision trees, and are flexible models that don't need much data preprocessing. There are two main differences:\n",
    "\n",
    "- In gradient boosting, trees are built one at a time, such that successive weak learners learn from the mistakes of preceding weak learners. In random forests, the trees are built independently at the same time.\n",
    "\n",
    "- The output of gradient boosting combines the results of the weak learners with each successive iteration, whereas in random forests, the trees are combined at the end (through averaging or majority).\n",
    "\n",
    "Because of these differences, gradient boosting is often more prone to overfitting than are random forests due to their focus on mistakes over training iterations and the lack of independence in tree building. Gradient boosting hyperparameters are harder to tune than those of random forests, and may take longer to train than random forests because the trees are built sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce656c-a8f0-4df0-80c3-4e2e496f8f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8849e40e-e0b0-4019-8284-6a62fb38f28f",
   "metadata": {},
   "source": [
    "<h4>Question 11</h4>\n",
    "\n",
    "<p>Say DoorDash is launching in Singapore. For this new market, you want to predict the ETA for a delivery to reach a customer after an order has been placed on the app. From an earlier beta test, there were $10,000$ delvieries made. Do you have enough training data to create an accurate ETA model?</p>\n",
    "\n",
    "<i>Answer:</i>\n",
    "<ol>\n",
    "<li>Step 1: Clarify what \"good\" [ETA] means</li>\n",
    "    <ul>\n",
    "        <li>How accurate does the ETA model need to be? The accuracy needed might be higher for the order-driver matching algorithm than what DoorDash needs to display to the customer. Also, it might be acceptable to under-promise and over-deliver.</li>\n",
    "    </ul>\n",
    "</br>\n",
    "<li>Step 2: Assess baseline [ETA] performance</li>\n",
    "    <ul>\n",
    "        <li>A baseline model can be something as simple as the estimated driving time plus average preparation time</li>\n",
    "    </ul>\n",
    "</br>\n",
    "<li>Step 3: Determine how more data improves accuracy</li>\n",
    "    <ul>\n",
    "        <li>A learning curve depicts how the accuracy changes when we train a model on a progressively larger percentage of the data. The point at which a metric like $R^2$ drops significantly compared to using less data is a signal to start re-evaluating features rather than adding data.</li>\n",
    "    </ul>\n",
    "</br>\n",
    "<li>Step 4: In case performance isn't good enough</li>\n",
    "    <ul>\n",
    "        <li>Are there too few features? Maybe add market supply and demand indicators, traffic patterns, etc.</li>\n",
    "        <li>Are there too many features, causing the model to overfit? e.g., are there as many as there are data points? If so, consider dimension reduction or feature selection techniques.</li>\n",
    "        <li>Will different models better-handle the smaller data?</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e46a15-08ee-42df-b3c2-54a848aff79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6ce7a8-1ee0-48bd-bba2-5883ebaa7b00",
   "metadata": {},
   "source": [
    "<h4>Question 12</h4>\n",
    "\n",
    "Say we are running a binary classification loan model, and rejected applicants must be supplied with a reason for rejection. Without digging into the weights of the features, how would you supply these reasons?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "We could look at partial dependence plots (a.k.a. response curves) to assess how any one feature affects the model's decision. A PDP shows the marginal effect of a feature on the predicted target of a machine learning model.\n",
    "\n",
    "Suppose we have the following situation:\n",
    "- $100K$ income, $10K$ debt, $2$ credit cards, FICO score $700$\n",
    "- $100K$ income, $10K$ debt, $2$ credit cards, FICO score $720$\n",
    "- $100K$ income, $10K$ debt, $2$ credit cards, FICO score $600$\n",
    "- $100K$ income, $10K$ debt, $2$ credit cards, FICO score $650$\n",
    "$\n",
    "If the third and fourth instances were rejected but instances one and two were accepted, we can conclude that (since all-else is equal) this is because of a FICO score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258bda1-cabb-4771-9e6b-80521617aaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7403a65-60f2-4502-8e22-b26b4572a358",
   "metadata": {},
   "source": [
    "<h4>Question 13</h4>\n",
    "\n",
    "Say you have a large corpus of words. How would you identify synonyms?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "We can find word embeddings (e.g., using Word2Vec) which produces vectors based on words' contexts. Once we have these, we can run an algorithm like k-means clustering or KNN to find particular synonyms for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651677f-7894-4237-8a4b-ce6b10d2eea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe0ba05-3235-46d0-a5a9-f641313ffe50",
   "metadata": {},
   "source": [
    "<h4>Question 14</h4>\n",
    "\n",
    "What is the bias-variance trade-off? How can it be expressed using an equation?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "The bias-variance trade-off can be expressed as:\n",
    "\n",
    "$\\text{Total Model Error} = \\text{Bias} + \\text{Variance} + \\text{Irreducible Error}$\n",
    "\n",
    "Flexible models tend to have low bias and high variance, and more rigid models do the opposite.\n",
    "\n",
    "The bias term comes from the error that occurs when a model underfits data (is too simple) and the variance term represents the error that occurs when a model overfits data, i.e., is too susceptible to changes in training data.\n",
    "\n",
    "When creating a machine learning model, we want both bias and variance to be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf437e-9149-4dbc-9491-7d36ee8b4b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be60a44d-2779-4630-a88d-346628a0a96c",
   "metadata": {},
   "source": [
    "<h4>Question 15</h4>\n",
    "\n",
    "Describe cross-validation and the motivation behind it.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Cross-validation assesses the performance of an algorithm in several resamples of the training data. It evaluates model performance on the portion of data not present in the subsample. The process is to:\n",
    "\n",
    "1. Randomly shuffle data into $k$ equally sized blocks\n",
    "\n",
    "2. For each $i$ in fold $1, \\ldots, k$, train the model on all the data except for field $i$, and evaluate the validation error using block $i$.\n",
    "\n",
    "3. Average the $k$ validation errors from step 2 to get an estimate of the true error.\n",
    "\n",
    "This accomplishes the following:\n",
    "- Avoids training and testing on the same subset of points\n",
    "- Avoids using a dedicated validation set, with which no training can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37254b57-8005-470d-b4a0-8cc31254dad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3e76586-5ea1-4fa6-b66a-8f791a15c21c",
   "metadata": {},
   "source": [
    "<h4>Question 16</h4>\n",
    "\n",
    "How would you build a lead scoring algorithm to predict whether a prospective company is likely to convert into being an enterprise customer?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Step 1: Clarify lead scoring requirements\n",
    "\n",
    "- Are we building this for our own company's sales leads, or an extensible version of our product?\n",
    "- Bus requirements (e.g., does it need to be easy to explain internally and externally)\n",
    "- Are we running the algorithm only on our sales database, or a larger landscape of all companies?\n",
    "\n",
    "Step 2: Explain the features you would use\n",
    "\n",
    "Elements which should influence whether a prospective company turns into a customer\n",
    "- Firmographic data: what type of company, industry, amount of revenue, or employee count?\n",
    "- Marketing data: have they interacted with marketing materials, like clicking on email-campaign links?\n",
    "- Sales activity: has the prospective company interacted with sales? How many meetings took place, and how recent was the last one?\n",
    "- Deal details: what products/licenses/etc. are being bought. What's the size of the deal/contract?\n",
    "- Then feature engineering upon these features\n",
    "\n",
    "Step 3: Explain models you would use\n",
    "\n",
    "- Lead scoring can be done with a binary classifier that predicts the probability of a lead occurring. Logistic regression has an easily interpretable result. The log-odds is a probability score for purchasing a particular item. But it cannot capture complex interactions and can be numerically unstable under conditions like correlated covariates.\n",
    "\n",
    "- A compromise between logistic regression and less interpretable models like neural networks or SVMs would be tree-based models, in which feature importance can readily be measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327946d-78aa-422b-8694-116933e15927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78c41df0-d7c0-4911-b3fd-42d22ecd5225",
   "metadata": {},
   "source": [
    "<h4>Question 19</h4>\n",
    "\n",
    "Explain what information gain and entropy are in the context of a decision tree.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Information gain is based on entropy, so we'll start with that:\n",
    "\n",
    "$Entropy = \\sum_{i=1} - P(Y=k) ~log ~P(Y=k)$\n",
    "\n",
    "The amount of entropy shows how homogenous a sample is. A completely homogenous sample will have an entropy of $0$.\n",
    "\n",
    "Information gain is based on the decrease in entropy ($H$) after splitting on an attribute.\n",
    "\n",
    "$IG(X_j, Y) = H(Y) - H(Y|X_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489a35f-49ef-4dbc-ae5e-9a58fb12b7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a7c739-4177-4328-a487-2186d1ec3c02",
   "metadata": {},
   "source": [
    "<h4>Question 20</h4>\n",
    "\n",
    "What is $L1$ and $L2$ regularization, and the difference between them?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Both methods prevent overfitting by coercing the coefficients of a regression model toward zero.\n",
    "\n",
    "$L1$, or Lasso, adds the absolute value of the coefficients as a penalty term, whereas 'ridge' $L2$ regularization adds the squared magnitude of the coefficients as the penalty term.\n",
    "\n",
    "$Loss(L1) = L + \\lambda |w_i|$\n",
    "\n",
    "$Loss(L2) = L + \\lambda |w_i^2|$\n",
    "\n",
    "where $L = \\sum_{i=1}^n (y_i - f(x_i))^2$\n",
    "\n",
    "$L1$ forces any weight closer to zero, irrespective of magnitude, whereas with $L2$, the rate at which the weight approaches zero becomes slower as the weight approaches zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0ab81-a5ad-427b-b279-af7b12f03c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0770f7-a68a-4a0d-b944-79142572f1da",
   "metadata": {},
   "source": [
    "<h4>Question 21</h4>\n",
    "\n",
    "Describe gradient descent and the motivations behind SGD.\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "The gradient descent algorithm takes small steps in the direction of steepest descent to optimize a particular objective function. The size of the steps the algorithm takes are proportional to the negative gradient of the function at the current value of the parameter being sought.\n",
    "\n",
    "The stochastic version of the algorithm, SGD, uses an approximation of the gradient by using only 1 randomly selected sample at each step to calculate the derivative of the function, making this version of the algorithm faster and more attractive for situations involving lots of data.\n",
    "\n",
    "The gradient descent algorithm will update x as follows until it reaches convergence.\n",
    "\n",
    "$x^{t+1} = x^t - \\alpha_t \\nabla f(x')$\n",
    "\n",
    "i.e., we calculate the negative of the gradient of $f$ and scale that by some constant and move in that direction at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605580a-279d-4e39-b84b-b09122c9cad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "918da966-6c06-495e-a13e-831808fe90f3",
   "metadata": {},
   "source": [
    "<h4>Question 22</h4>\n",
    "\n",
    "Assume we have a classifier that produces a score between 0 and 1 for the probability of a particular loan being fraudulent. How would the ROC curve change if we took the square root of that score? If it doesn't change, what kind of functions would change the curve?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "ROC curves plot the TPR vs. the FPR. If all scores change simultaneously, then none of the actual classifications change, leading to the same TPR and FPR.\n",
    "\n",
    "In contrast, any function that is not monotonically increasing would change the ROC curve, since the relative ordering would not be maintained.\n",
    "\n",
    "e.g., $f(x)=-x$, $f(x)=-x^2$, or a stepwise function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8ff8d-3215-4ef6-aad6-dd30a161e2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f8ab79-a7d8-434f-91cd-a70de4782f23",
   "metadata": {},
   "source": [
    "<h4>Question 25</h4>\n",
    "\n",
    "Compare and contrast Gaussian Naive Bayes and logistic regression. When would you use one over the other?\n",
    "\n",
    "<i>Answer:</i>\n",
    "\n",
    "Advantages:\n",
    "- GNB requires only a small number of observations to be adequately trained. It is also easy to use and reasonably fast to implement.\n",
    "- Logistic regression has a simple interpretation in terms of class probabilities, and it allows inferences to be made about features and identification of feature importance.\n",
    "\n",
    "Disadvantages:\n",
    "- GNB assumes the variables are independent\n",
    "- Logistic regression is not highly flexible. It may fail to capture interactions between features and so it may lose predictive power. The lack of flexibility can also lead to overfitting.\n",
    "\n",
    "Differences:\n",
    "- Since logistic regression directly learns P(Y|X), it is a discriminative classifier, whereas GNB directly estimates $P(Y)$ and $P(X|Y)$ and so it is a generative classifier.\n",
    "\n",
    "Similarities:\n",
    "- Both methods are linear decision functions generated from training data.\n",
    "- GNB's implied $P(Y|X)$ is the same as that of logistic regression (but with particular parameters).\n",
    "\n",
    "Logistic regression would be preferable assuming data size is not an issue, since the assumption off conditional independence breaks down if features are correlated. However, in cases where training data are limited or the data-generating process includes strong priors, using GNB may be preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe009e5-b874-4ff2-81b9-000f800d41c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ccd719-9e3d-49b0-8bc1-031fcb1bfef7",
   "metadata": {},
   "source": [
    "<h4>Question 27</h4>\n",
    "\n",
    "Describe the kernel trick in SVMs and give a simple example. How do you decide which kernel to use?\n",
    "\n",
    "The idea behind the kernel trick is that data that cannot be separated by a hyperplane in current dimensionality can be separable by projecting it onto a higher dimensional space.\n",
    "\n",
    "$k(x,y) = \\Phi(x)^T \\phi(y)$\n",
    "\n",
    "Say we have two examples and want to map them to a quadratic space. We have the following:\n",
    "\n",
    "$\\Phi(x_1,x_2) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1 x_2 \\\\ \\end{bmatrix}$\n",
    "\n",
    "and we can use the following:\n",
    "\n",
    "$k(x,y) = (1 + x^Ty)^2 = \\Phi(x)^T \\Phi(y)$\n",
    "\n",
    "If we now change $n=2$ (quadratic) to arbitrary $n$, we can have arbitrarily complex $\\Phi$. As long as we perform computations in the original feature space (without feature transformation), then we avoid long compute time while still mapping to a higher dimension.\n",
    "\n",
    "For linear problems, we can try a linear or logistic kernel. For nonlinear problems, we can try either the radial basis function (RBF) or Gaussian kernels. We could also try many kernels along with a grid search of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441b389-9c5a-4d89-b07e-8ff2a4bd9cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a49d397f-539e-4df1-95ab-4e7ae5d53487",
   "metadata": {},
   "source": [
    "<h4>Question 32</h4>\n",
    "\n",
    "Describe the idea behind PCA and describe its formulation in matrix form. Go through the procedural description.\n",
    "- PCA aims to reconstruct data into a lower dimensional setting, and so it creates a small number of linear combinations of a vector $x$ (of $p$ dimensionality) to explain the variance within $x$. We want to find the vector $w$ of weights such that we can define the following linear combination.\n",
    "\n",
    "$y_i = w_i^Tx = \\sum_{j=1}^p w_{ij} x_j$\n",
    "\n",
    "subject to the constraint that w is orthonormal and that $y_i$ is uncorrelated with $y_j$, and $Var(y_i)$ is maximized.\n",
    "\n",
    "We perform the following procedure, in which we first find $y_1 = w_1^Tx$ with maximal variance, meaning that the series are obtained by orthogonally projecting the data onto the first principal direction, $w_1$. We then find $y_2 = w_2^Tx_1$ is uncorrelated with $y_1$ and has maximal variance, and we continue this procedure iteratively until ending with the $k^{th}$ dimension such that $y_1, \\ldots, y_k$ explain the majority of variance, $k \\lt p$.\n",
    "\n",
    "To solve, not we havec the following for the variance of each $y$, utilizing the covariance matrix of $x$.\n",
    "\n",
    "$var(y_i) = w_i^T var(x) w_i = w_i^T \\sum w_i$\n",
    "\n",
    "Without any constraints, we could choose arbitrary weights to masximize this variance, and hence we will normalize by assuming orthonormality of $w$, which guarantees that $w_i^T w_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac780c5e-fb54-49af-b59b-bf0d73458a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca4fc7e4-66e6-4d84-ae5f-3a89d29ed430",
   "metadata": {},
   "source": [
    "<h4>Question 34</h4>\n",
    "\n",
    "How would you approach creating a music recommendation algorithm for Discover Weekly (a 30-song weekly playlist personalized to an individual user)?\n",
    "\n",
    "Step 1: Clarify Details of Discover Weekly\n",
    "- What is the goal of the algorithm?\n",
    "- Do we recommend just songs, or include podcasts?\n",
    "- Is the goal to recommend new music?\n",
    "- Does the playlist need to change week-to-week if the user doesn't listen to it?\n",
    "\n",
    "Step 2: Describe What Data Features You Would Use\n",
    "- User-song interactions\n",
    "- Metadata about the song\n",
    "- Audio features\n",
    "- Demographic features, e.g., region\n",
    "\n",
    "Step 3: Explain Collaborative Filtering Model Setup\n",
    "- Collaborative filtering uses data from feedback users have provided. A user-song matrix constitutes the dataset, and since explicit song ratings are lacking, we can proxy liking a song by using the number of times a user streamed it.\n",
    "- Similarity measurement, KNN...\n",
    "\n",
    "Step 4: Additional Considerations\n",
    "- Cold start problem\n",
    "- Scale\n",
    "- Retraining \n",
    "- How to measure/track system over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
